# MVP
- V define dataset manifest formats and schemas
- V setup verbose logging
- V implement root poll command with passing a manifest in
- V ftp support
- V integrate transform
- V Multi-file support
- V Get rid of `spark-warehouse` folder
- V Replace cache json with yaml
- V Use docker to run spark
- V Put GeoSpark into image, not dependencies
- V Create PySpark + Livy image
- V Create Jupyter + SparkMagic + Kamu image
- V Create `notebook` command to launch Jupyter
- V Permission fix-up for `notebook` command
- V Rethink repository structure and how datasets are linked
- V Implement `pull` command to refresh dataset
- V Implement `list` command showing datasets and their metadata
- V DOT graph output
- V Referential integrity
- V `delete` command
- V `purge` command
- V Root dataset creation wizard (generate based on schema)
- V Control output verbosity
- V Add `sql --server` command that starts Livy with JDBC endpoint exposed
- V Add `sql` interactive shell
- V Add support for one-off `sql` commands
- V Add command to pull docker images (`init --pull-images`)
- V Host images on docker hub
- V Livy sessions should be pre-configured with GeoSpark types
- V Add a version information
- V Distribution `mac`
- V Prettify `help`
- Simplify historical vocab
- Rethink repo structure (again!)
- Distribution `arch`
- Distribution `ubuntu`
- Write version metainfo for later upgrades
- Cleanup poll data
- Create "stable" repository of known good datasources
- Allow adding manifest from remote URL
- Consider `brew create` style of generating new sources (include enough comments to guide the author)
- Simplify build
- Homebrew formula should build from source

# Post-MVP
- V Suppress HDFS warning on startup
- V Make list of environment vars propagated to notebooks configurable
- More flexible source configuration
- Implement recursive mode for `pull` command
- Handle dependencies during `purge`
- Implement `describe` command showing dataset metadata
- Implement `status` command showing which data is out-of-date
- Force `pull` to update uncacheable datasets
- Add bash completions
- Standardize tabular format and output options
- Figure out "unable to infer schema" when loading folder of parquets
- Implement file `import` and `export`
- Add `spark-shell` command (scala and python)
- Upgrade to latest Spark
- Avoid permission issues by propagating UID/GID into containers
- Lock repository where necessary to prevent concurrent alteration
- Avoid hard-setting the container names
- Unify log formats of all tools
- Use local timezone in all logs
- Allow using custom-built docker images
- Add heuristics to help with configuring dataset sources

# Known issues
- V `NullPointerException` when creating temporary view in sql shell
- V `NoSuchElementException` on every SQL syntax error instead of root cause
- `Mounts denied` error on Mac when mounting `/usr/local/Cellar/kamu/x.x.x/bin/kamu`
- `Ctrl+C` kills connection in SQL shell
- Spamy warning in Spark `Truncated the string representation of a plan since it was too large`.
- ^ MVP Blockers ^
- `dynver` plugin is not tagging snapshot versions as advertised
- `sparkmagic` spams warnings in notebooks
- `Kryo serialization failed: Buffer overflow` when manipulating large GIS data
- `sqlline version ???` uglyness on sql shell startup
- `Transaction isolation level` warning on sql shell startup
- `sparkmagic` garbles some dataset types and often requires casting to string to preserve data
- `No data or no sasl data in the stream` error in Livy logs due to helth check
